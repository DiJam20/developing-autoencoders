{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import umap\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "from solver import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True, num_workers=6)\n",
    "\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_type, epoch):\n",
    "    n_input = 28*28\n",
    "    n_layers = 3\n",
    "    sae_n_hidden_ls = [512, 128, 32]\n",
    "\n",
    "    size_ls = [4, 4, 4, 4, 4, 10,\n",
    "            10, 10, 10, 10, 16, 16,\n",
    "            16, 16, 16, 16, 16, 24,\n",
    "            24, 24, 24, 24, 24, 24, \n",
    "            32, 32, 32, 32, 32, 32,\n",
    "            32, 32, 32, 32, 32, 32, \n",
    "            32, 32, 32, 32, 32, 32, \n",
    "            32, 32, 32, 32, 32, 32, \n",
    "            32, 32]\n",
    "    \n",
    "    dae_n_hidden_ls = [512, 128, size_ls[epoch]]\n",
    "    \n",
    "    if model_type == 'SAE':\n",
    "        model = Autoencoder(n_input, sae_n_hidden_ls, n_layers)\n",
    "    else:\n",
    "        model = Autoencoder(n_input, dae_n_hidden_ls, n_layers)\n",
    "    weights = torch.load(f\"{model_path}/{model_type}/model_weights_epoch{epoch}.pth\")\n",
    "    model.load_state_dict(weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63930/2098548407.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(f\"{model_path}/{model_type}/model_weights_epoch{epoch}.pth\")\n"
     ]
    }
   ],
   "source": [
    "modelpath = f'/home/david/sparsify_models/good_models/01'\n",
    "\n",
    "sae = load_model(modelpath, 'SAE', epoch=49)\n",
    "dae = load_model(modelpath, 'DAE', epoch=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    train_images.append(data)\n",
    "    train_labels.append(target)\n",
    "\n",
    "train_images = np.concatenate(train_images)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    data = Variable(data)\n",
    "    test_images.append(data)\n",
    "    test_labels.append(target)\n",
    "\n",
    "test_images = np.concatenate(test_images)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # take one test image. encode it. add noise to the first element of the encoding. decode it.\n",
    "    sample_image = test_images[0]\n",
    "    sample_image = torch.tensor(sample_image).to('cuda')\n",
    "    input = sample_image.view(1, -1).to('cpu')\n",
    "    encoded = dae.encode(input)\n",
    "    encoded[0][0] = 0\n",
    "    decoded = dae.decode(encoded)\n",
    "    decoded = decoded.view(28, 28).cpu().numpy()\n",
    "    plt.imshow(decoded, cmap='gray')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_test_encodings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image in test_images:\n",
    "        image = torch.tensor(image).view(-1)\n",
    "        input = image.view(1, -1)\n",
    "        encoded, decoded = dae(input)\n",
    "        dae_test_encodings.append(encoded)\n",
    "\n",
    "dae_test_encodings = np.concatenate(dae_test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae_test_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      3\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pca_result \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:511\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/utils/validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m             )\n\u001b[0;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(torch.tensor(test_images).view(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
